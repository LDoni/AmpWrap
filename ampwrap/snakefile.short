import os
import glob
import datetime

# Directories and parameters
start_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
input_dir = config["input_dir"]
output_dir = config["output_dir"]
sample_tab = config["sample_tab"]
file_extension = config["file_extension"]
db_dir = config.get("db_dir","db_s")
forward_primer = config["forward_primer"]
reverse_primer = config["reverse_primer"]
amplicon_length = config["amplicon_length"]
files = config["sample"]
threads = config.get("threads",os.cpu_count())
figaroname = "fvieira"
discard_untrimmed = "--discard-untrimmed"
disable_discard_untrimmed = config["disable_discard_untrimmed"]
if disable_discard_untrimmed: discard_untrimmed = ""
dada2_params = config["dada2_params"]
param_source = config["dada2_source"]
dry_run_tune = config.get("dry_run_tune", False)




CONDA_BIN = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin')
SCRIPTS_DIR = os.path.join(CONDA_BIN, 'scripts') if os.path.exists(os.path.join(CONDA_BIN, 'scripts')) else 'scripts'


SCRIPT_PATHS = {
    'figaro': os.path.join(SCRIPTS_DIR, 'figaro/figaro/figaro.py'),
    'dada2_filter': os.path.join(SCRIPTS_DIR, 'dada2_filter.R'),
    'dada2_learn_errors': os.path.join(SCRIPTS_DIR, 'dada2_learn_errors.R'),
    'dada2_infer_asvs': os.path.join(SCRIPTS_DIR, 'dada2_infer_asvs.R'),
    'dada2_remove_chimeras': os.path.join(SCRIPTS_DIR, 'dada2_remove_chimeras.R'),
    'dada2_assign_taxonomy': os.path.join(SCRIPTS_DIR, 'dada2_assign_taxonomy.R'),
    'dada2_assign_taxonomy2': os.path.join(SCRIPTS_DIR, 'dada2_assign_taxonomy2.R'),
    'track_reads': os.path.join(SCRIPTS_DIR, 'track_reads.R'),
    'reporter': os.path.join(SCRIPTS_DIR, 'reporter_short.py')
}


for name, path in SCRIPT_PATHS.items():
    if not os.path.exists(path):
        raise RuntimeError(f"Script {name} not found at {path}")


def sampleInfos(sample_config):
        d = {}
        for i,l in enumerate(open(sample_config)):
               # print(l)
                if l.startswith("#"): continue
                sample,index,file,filenaming = l.strip().split()
                if not index.lower().startswith("r"): index = f"r{index}"
                d[sample] = d.get(sample,[])
                d[sample].append((file,index,filenaming))
        return d

sample_d = sampleInfos(sample_tab)
samples = sample_d.keys()
indices = [1,2]

# Compute primer lengths
forward_primer_length = len(forward_primer)
reverse_primer_length = len(reverse_primer)

# Create SymLinks
def rename_samples(samples):
    # get sample names compatible for FVieira format
    renamed_dict = {}
    seen_names = set()
    for sample in samples:
        new_name = sample.replace("_", "-")
        base_name = new_name
        counter = 1
        while new_name in seen_names:
            new_name = f"{base_name}-{counter}"
            counter += 1
        renamed_dict[sample] = new_name
        seen_names.add(new_name)
    return renamed_dict

sample_renamed_dict = rename_samples(sample_d.keys())
renamed_samples2files = {}
for sample,infos in sample_d.items():
    renamed_sample = sample_renamed_dict[sample]
    renamed_samples2files[renamed_sample] = renamed_samples2files.get(renamed_sample,{})
    for i in infos:
        f,index = i[0],i[1]
        renamed_samples2files[renamed_sample][index] = f

# tax db
assign_taxonomy_method = config["assign_taxonomy_method"]
out_name = {
    "dada2_silva_genus138": "silva_nr99_v138.2_toGenus_trainset.fa.gz",
    "dada2_RDP_genus19": "rdp_19_toGenus_trainset.fa.gz",
    "dada2_GG2_genus09": "gg2_2024_09_toGenus_trainset.fa.gz",
    "dada2_RefSeq_RDPv16": "RefSeq_16S_6-11-20_RDPv16_Genus.fa.gz",
    "dada2_GTDB_r202": "GTDB_bac120_arc122_ssu_r202_Genus.fa.gz",
    "decipher_silva138": "SILVA_SSU_r138_2_2024.RData",
    "decipher_gtdb226": "GTDB_r226-mod_April2025.RData",
    "decipher_rdp18": "RDP_v18-mod_July2020.RData"
}.get(assign_taxonomy_method)

# functions for inputs
def getReadFiles(wildcards,d=sample_d):
    #print([i[0] for i in d[wildcards.sample]])
    return [i[0] for i in d[wildcards.sample]]

def getReadFilesDict(wildcards,d=sample_d):
    out = {i[1].lower():i[0] for i in d[wildcards.sample]}
    #print(out)
    return out


rule all:
    input:
        # se dry-run-tune, esegue solo figaro
        expand(
            f"{config['output_dir']}/intermediate/figaro/trimParameters.json",
        ) if dry_run_tune else
        # altrimenti lista completa degli output
        [
            # figaro input
            *expand(f"{config['output_dir']}/figaro_inp/{{sample}}_R{{index}}.{{extension}}",
                    sample=sample_renamed_dict.values(), index=indices, extension=file_extension),
            # FastQC iniziale
            *expand(f"{config['output_dir']}/QC/raw_qc_initial/{{file}}_fastqc.html", file=files),
            f"{config['output_dir']}/QC/raw_qc_initial/multiqc_report.html",
            # Cutadapt
            *expand(f"{config['output_dir']}/intermediate/cutadapt/{{sample}}_trimmed_R{{index}}.fq.gz", sample=samples, index=indices),
            *expand(f"{config['output_dir']}/QC/raw_qc_post_cutadapt/{{sample}}_trimmed_R{{index}}_fastqc.html", sample=samples, index=indices),
            f"{config['output_dir']}/QC/raw_qc_post_cutadapt/multiqc_report.html",
            f"{config['output_dir']}/intermediate/figaro/trimParameters.json",
            f"{config['output_dir']}/intermediate/cutadapt/cutadapt_primer_trimming_stats.txt",
            # DADA2
            *expand(f"{config['output_dir']}/intermediate/dada2_filtered/{{sample}}_R{{index}}_filtered.fq.gz", sample=samples, index=indices),
            f"{config['output_dir']}/intermediate/dada2_error_learning/err_forward_reads.rds",
            f"{config['output_dir']}/intermediate/dada2_error_learning/err_reverse_reads.rds",
            f"{config['output_dir']}/intermediate/merged_asvs.rds",
            f"{config['output_dir']}/intermediate/no_chimera_asvs.rds",
            f"{db_dir}/dada2/{out_name}",
            f"{config['output_dir']}/results/ASVs.fa",
            f"{config['output_dir']}/results/ASVs_counts.tsv",
            f"{config['output_dir']}/results/ASVs_taxonomy.tsv",
            f"{config['output_dir']}/results/read-count-tracking.tsv",
            f"{config['output_dir']}/results/final_report.txt"
        ]

rule input_symlink:
    input: renamed_samples2files[renamed_sample][index]
    output: f"{output_dir}/figaro_inp/{{sample}}_R{{index}}.{{extension}}"
    run:
        import os
        os.makedirs(f"{output_dir}/figaro_inp",exist_ok=True)
        src,target = os.path.abspath(str(input)),os.path.abspath(str(output))
        print(src,target)
        try: os.symlink(src, target)
        except Exception as E: print(E); raise Error

rule fastqc_initial:
    input:
        f"{input_dir}/{{file}}.{file_extension}"
    output:
        "{output_dir}/QC/raw_qc_initial/{file}_fastqc.html"
    threads: threads
    shell:
        """
        mkdir -p {output_dir}/QC/raw_qc_initial
        fastqc {input} -o $(dirname {output}) -t {threads} > /dev/null 2>&1
        """

rule multiqc_initial:
    input: expand("{output_dir}/QC/raw_qc_initial/{file}_fastqc.html", output_dir=output_dir,file=files)
    output: f"{output_dir}/QC/raw_qc_initial/multiqc_report.html"
    threads: threads
    shell:
        """
        multiqc $(dirname {output}) -o $(dirname {output}) > /dev/null 2>&1
        """


rule cutadapt:
    input:
        unpack(getReadFilesDict)
    output:
        r1_trimmed = "{output_dir}/intermediate/cutadapt/{sample}_trimmed_R1.fq.gz",
        r2_trimmed = "{output_dir}/intermediate/cutadapt/{sample}_trimmed_R2.fq.gz"
    log:
        "{output_dir}/intermediate/cutadapt/{sample}_cutadapt.log"
    params:
        forward_primer = forward_primer,
        reverse_primer = reverse_primer,
    shell:
        """
        mkdir -p {output_dir}/intermediate/cutadapt
        cutadapt -g "^{params.forward_primer}" -G "^{params.reverse_primer}" {discard_untrimmed} \
                 -o {output.r1_trimmed} -p {output.r2_trimmed} \
                 {input.r1} {input.r2} > {log}
        """

rule combine_cutadapt_logs:
    input: expand("{output_dir}/intermediate/cutadapt/{sample}_cutadapt.log", output_dir=output_dir, sample=samples)
    output: f"{output_dir}/intermediate/cutadapt/cutadapt_primer_trimming_stats.txt"
    log: f"{output_dir}/intermediate/cutadapt/combined_cutadapt.log"
    shell:
         f"""
        cat {{input}} > {{output}}
        echo -e "sample\treads retained\tbps retained" > {{log}}
        paste <(echo {" ".join(list(samples))} | tr " " "\n") <(grep "passing" {{output}} | cut -f3 -d "(" | tr -d ")") <(grep "filtered" {{output}} | cut -f3 -d "(" | tr -d ")") >> {{log}}
        cat {{log}}
        mkdir -p "{output_dir}/QC/raw_qc_post_cutadapt/"
        mv {{input}} "{output_dir}/QC/raw_qc_post_cutadapt/"
         """


rule fastqc_post_cutadapt:
    input:
        trimmed_files = "{output_dir}/intermediate/cutadapt/{sample}_trimmed_R{index}.fq.gz"
    output:
        fastqc = "{output_dir}/QC/raw_qc_post_cutadapt/{sample}_trimmed_R{index}_fastqc.html"
    threads: 15
    shell:
        """
        mkdir -p {output_dir}/QC/raw_qc_post_cutadapt
        fastqc {input.trimmed_files} -o $(dirname {output}) -t {threads}  > /dev/null 2>&1
        """


rule multiqc_final:
    input: expand("{output_dir}/QC/raw_qc_post_cutadapt/{sample}_trimmed_R{index}_fastqc.html", output_dir=output_dir,sample=samples,index=indices)
    output: f"{output_dir}/QC/raw_qc_post_cutadapt/multiqc_report.html"
    threads: threads
    shell:
        """
        multiqc $(dirname {output}) -o $(dirname {output})  > /dev/null 2>&1
        """


rule figaro:
    input:
        expand(f"{config['output_dir']}/figaro_inp/{{sample}}_R{{index}}.{{extension}}",
               sample=sample_renamed_dict.values(), index=indices, extension=file_extension)
    output:
        trimParameters = config["output_dir"] + "/intermediate/figaro/trimParameters.json"
    params:
        amplen = amplicon_length,
        forward_primer_length = forward_primer_length,
        reverse_primer_length = reverse_primer_length,
        input_dir = f"{output_dir}/figaro_inp",
        figaroname = figaroname,
        figaro_script = SCRIPT_PATHS['figaro']
    run:
        import subprocess
        import os
        import json
        import sys

        # crea la cartella
        os.makedirs(f"{output_dir}/intermediate/figaro", exist_ok=True)

        # esegui Figaro in modo silenzioso
        subprocess.run([
            "python", params.figaro_script,
            "-i", params.input_dir,
            "-o", f"{output_dir}/intermediate/figaro",
            "-a", str(params.amplen),
            "-f", str(params.forward_primer_length),
            "-r", str(params.reverse_primer_length),
            "-F", params.figaroname
        ], check=True)

        # se dry-run, stampa solo prime 5 righe come tabella
        if dry_run_tune:
            try:
                with open(output.trimParameters) as f:
                    # Leggi l'intero file come array JSON
                    records = json.load(f)
                    
                    # Verifica che sia una lista
                    if not isinstance(records, list):
                        records = [records]
                    
                    # Prendi solo i primi 5 record
                    records = records[:5]

            except json.JSONDecodeError as e:
                print(f"Error reading JSON file: {e}", file=sys.stderr)
                records = []

            # headers
            headers = ["trimPosition", "maxExpectedError", "readRetentionPercent", "score"]
            print(" | ".join(headers))
            print("-" * 60)

            for rec in records:
                # Verifica che il record sia un dizionario
                if not isinstance(rec, dict):
                    print(f"Warning: Skipping non-dictionary record: {rec}", file=sys.stderr)
                    continue
                
                # Usa get con valori di default
                trim_pos = rec.get('trimPosition', [])
                max_err = rec.get('maxExpectedError', [])
                read_ret = rec.get('readRetentionPercent', 0)
                score = rec.get('score', 0)
                
                # Verifica che siano liste (per sicurezza)
                if not isinstance(trim_pos, list):
                    trim_pos = [trim_pos]
                if not isinstance(max_err, list):
                    max_err = [max_err]
                
                print(f"{repr(trim_pos)} | {repr(max_err)} | {read_ret} | {score}")






rule dada2_filter:
    input:
        figaro_params=f"{output_dir}/intermediate/figaro/trimParameters.json"
    output:
        filtered_files=expand(
            f"{output_dir}/intermediate/dada2_filtered/{{sample}}_R{{index}}_filtered.fq.gz",
            sample=samples,
            index=indices),
        checkout=f"{output_dir}/intermediate/dada2_filtered/checkout.filter"
    params:
        filter_script = SCRIPT_PATHS['dada2_filter'],
        dada2_params = config.get("dada2_params")
    shell:
        """
        mkdir -p {output_dir}/intermediate/dada2_filtered
        Rscript {params.filter_script} \
        {output_dir}/intermediate/cutadapt {output_dir}/intermediate/dada2_filtered {input.figaro_params} {params.dada2_params}

    touch {output.checkout}
        """


rule dada2_error_learning:
    input:
        f"{output_dir}/intermediate/dada2_filtered/checkout.filter"
    output:
        expand("{output_dir}/intermediate/dada2_error_learning/err_{direction}_reads.rds", output_dir=output_dir,direction=["forward", "reverse"])
    params:
        learn_script = SCRIPT_PATHS['dada2_learn_errors']
    shell:
        """
        mkdir -p {output_dir}/intermediate/dada2_error_learning
        Rscript {params.learn_script} \
        {output_dir}/intermediate/dada2_filtered {output_dir}/intermediate/dada2_error_learning
        """

rule dada2_infer_asvs:
    input:
        err_fwd= "{output_dir}/intermediate/dada2_error_learning/err_forward_reads.rds",
        err_rev= "{output_dir}/intermediate/dada2_error_learning/err_reverse_reads.rds"
    output:
        merged_asvs = "{output_dir}/intermediate/merged_asvs.rds"
    params:
        infer_script = SCRIPT_PATHS['dada2_infer_asvs']
    shell:
        """
        Rscript {params.infer_script} {output_dir}/intermediate/dada2_filtered {input.err_fwd} {input.err_rev} {output.merged_asvs}
        """

rule dada2_remove_chimeras:
    input:
        merged_asvs= config["output_dir"] + "/intermediate/merged_asvs.rds"
    output:
        no_chimera_asvs = config["output_dir"] + "/intermediate/no_chimera_asvs.rds"
    params:
        chimera_script = SCRIPT_PATHS['dada2_remove_chimeras']
    shell:
        """
        Rscript {params.chimera_script} {input.merged_asvs} {output.no_chimera_asvs}
        """

rule download_dada2_db:
    output:
        dada2_db=f"{db_dir}/dada2/{out_name}"
    params:
        method=assign_taxonomy_method
    shell:
        """
        DB_DIR="$(dirname {output.dada2_db})"
        DB_PATH={output.dada2_db}

        # Ensure the directory exists
        mkdir -p "$DB_DIR"

        # Define URLs and MD5s
        if [ "{params.method}" == "decipher_silva138" ]; then
            DB_URL="https://www2.decipher.codes/data/Downloads/TrainingSets/SILVA_SSU_r138_2_2024.RData"
            DB_MD5="4e272e39c2d71f5d3e7a31b00dbb1df4"
            DB_FILENAME="SILVA_SSU_r138_2_2024.RData"
        elif [ "{params.method}" == "decipher_gtdb226" ]; then
            DB_URL="https://www2.decipher.codes/data/Downloads/TrainingSets/GTDB_r226-mod_April2025.RData"
            DB_MD5="2aca8a1cfc4c8357a61eb51413f4e476"
            DB_FILENAME="GTDB_r226-mod_April2025.RData"
        elif [ "{params.method}" == "decipher_rdp18" ]; then
            DB_URL="https://www2.decipher.codes/data/Downloads/TrainingSets/RDP_v18-mod_July2020.RData"
            DB_MD5="e0e8ed5bc34b28ab416df2d7fc1568ec"
            DB_FILENAME="RDP_v18-mod_July2020.RData"
        elif [ "{params.method}" == "dada2_silva_genus138" ]; then
            DB_URL="https://zenodo.org/records/16777407/files/silva_nr99_v138.2_toGenus_trainset.fa.gz?download=1"
            DB_MD5="1764e2a36b4500ccb1c7d5261948a414"
            DB_FILENAME="silva_nr99_v138.2_toGenus_trainset.fa.gz"
        elif [ "{params.method}" == "dada2_RDP_genus19" ]; then
            DB_URL="https://zenodo.org/records/14168771/files/rdp_19_toGenus_trainset.fa.gz?download=1"
            DB_MD5="390b8a359c45648adf538e72a1ee7e28"
            DB_FILENAME="rdp_19_toGenus_trainset.fa.gz"
        elif [ "{params.method}" == "dada2_GG2_genus09" ]; then
            DB_URL="https://zenodo.org/records/14169078/files/gg2_2024_09_toGenus_trainset.fa.gz?download=1"
            DB_MD5="82a2571c9ff5009cbd2f3fded79069ed"
            DB_FILENAME="gg2_2024_09_toGenus_trainset.fa.gz"
        elif [ "{params.method}" == "dada2_RefSeq_RDPv16" ]; then
            DB_URL="https://zenodo.org/records/4735821/files/RefSeq_16S_6-11-20_RDPv16_Genus.fa.gz?download=1"
            DB_MD5="53aac0449c41db387d78a3c17b06ad07"
            DB_FILENAME="RefSeq_16S_6-11-20_RDPv16_Genus.fa.gz"
        elif [ "{params.method}" == "dada2_GTDB_r202" ]; then
            DB_URL="https://zenodo.org/records/4735821/files/GTDB_bac120_arc122_ssu_r202_Genus.fa.gz?download=1"
            DB_MD5="40c1ee877ad2c5dca81e1cdf9a52ac3a"
            DB_FILENAME="GTDB_bac120_arc122_ssu_r202_Genus.fa.gz"
        else
            echo "Error: database method not recognized."
            exit 1
        fi

        # Download the database if it doesn't already exist
        if [ ! -f "$DB_PATH" ]; then
            echo "Downloading $DB_FILENAME..."
            wget -O "$DB_PATH" "$DB_URL"
            if [ $? -ne 0 ]; then
                echo "Error downloading $DB_FILENAME."
                exit 1
            fi
        fi

        # Check MD5
        echo "Checking MD5 checksum..."
        MD5_CALC=$(md5sum "$DB_PATH" | awk '{{print $1}}')
        if [ "$MD5_CALC" != "$DB_MD5" ]; then
            echo "MD5 mismatch for $DB_FILENAME"
            echo "Expected: $DB_MD5"
            echo "Got:      $MD5_CALC"
            exit 1
        else
            echo "MD5 verified for $DB_FILENAME"
        fi

        """



rule dada2_assign_taxonomy:
    input:
        no_chimera_asvs=config["output_dir"] + "/intermediate/no_chimera_asvs.rds",
        db=f"{db_dir}/dada2/{out_name}"
    output:
        asvs="{output_dir}/results/ASVs.fa",
        asvs_counts="{output_dir}/results/ASVs_counts.tsv",
        taxonomy="{output_dir}/results/ASVs_taxonomy.tsv",
    params:
        method=assign_taxonomy_method,
        tax_script1=SCRIPT_PATHS['dada2_assign_taxonomy'],
        tax_script2=SCRIPT_PATHS['dada2_assign_taxonomy2']
    shell:
        """
        if [[ {params.method} == "decipher_silva138" || {params.method} == "decipher_gtdb226" || {params.method} == "decipher_rdp18" ]]; then
            Rscript {params.tax_script2} {input.no_chimera_asvs} {input.db} {output_dir}/results
        elif [[ {params.method} == "dada2_silva_genus138" || {params.method} == "dada2_RDP_genus19" || {params.method} == "dada2_GG2_genus09" || {params.method} == "dada2_RefSeq_RDPv16" || {params.method} == "dada2_GTDB_r202" ]]; then
            Rscript {params.tax_script1} {input.no_chimera_asvs} {input.db} {output_dir}/results
        else
            echo "Error: method not valid."
            echo " try: ampwrap short --db-info"
            exit 1
        fi
        """

rule track_reads:
    input:
        merged_asvs=config["output_dir"] + "/intermediate/merged_asvs.rds",
        no_chimera_asvs=config["output_dir"] + "/intermediate/no_chimera_asvs.rds"
    output:
        track_reads=config["output_dir"] + "/results/read-count-tracking.tsv"
    params:
        track_script = SCRIPT_PATHS['track_reads']
    shell:
        """
        Rscript {params.track_script} \
        {output_dir}/intermediate/dada2_filtered/filter_summary.tsv {output_dir}/intermediate/dada_fwd.rds \
        {output_dir}/intermediate/dada_rev.rds {input.merged_asvs} {input.no_chimera_asvs} {output.track_reads}
        """

rule reporter:
    input:
        cutadapt = f"{config['output_dir']}/intermediate/cutadapt/combined_cutadapt.log",
        dada2 = f"{config['output_dir']}/results/read-count-tracking.tsv",
        figaro_json = f"{config['output_dir']}/intermediate/figaro/trimParameters.json"
    output:
        report = f"{config['output_dir']}/results/final_report.txt"
    params:
        forward_p = forward_primer,
        reverse_p = reverse_primer,
        start = start_time,
        # end = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        taxonomy_method = assign_taxonomy_method
    script:
        SCRIPT_PATHS['reporter']
